# mohamedsudheer

My approach is the following: Since we only care about the data from 2020 I create lists corresponding to the months and commoncrawl bucket names from Jan-Dec of 2020. I also created a simple list of 3 keywords which I used to check whether a certain page had relevant COVID-19 economic impact content or not. I start by iterating through my list of bucketnames and build the appropriate URL to that bucket's WET files. This is where I got stuck because I can't seem to figure out how to process this URL in the proper format to iterate through each line of the wet.paths.gz. I read online that the paths.gz link is formatted with a single warc.wet.gz path on each line. Therefore, I tried processing the URL by using info from here: https://stackoverflow.com/questions/10566558/python-read-lines-from-compressed-text-files but I got an error saying 'no such file or directory' so I'm assuming this method was meant for .gz files that were already downloaded locally. I also tried using request.get(myUrl).text to try and get that page data in text form but that method returned a bunch of tokenized data which was not processable in the manner I wanted. I spent some time experimenting with the request module by formatting my url in different ways but I did not have much luck. I believe the rest of my code should be okay however. Once I open the stream for the individual warc.wet.gz link I access the wet record by only looking at records whose rec_type == "conversion" since that was how commoncrawl described it. Once I verify the record type, I get all the contents of the record in text format and check if it contains all my keywords in which case I identify the page as relevant and print out its URL by referring to the WARC-Target-URI header. I figured that analyzing the WET files would be the easiest way to classify pages because I can directly compare the textual contents of the webpage to my keyword set to verify whether or not it contained relevant COVID-19 economic impact information.